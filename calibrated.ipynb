{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3690e52c",
   "metadata": {},
   "source": [
    "### Calibrated 3D Face Reconstruction from Stereo Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fd6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Matching pairs...\n",
      "Left-Front correspondences: 6111\n",
      "Front-Right correspondences: 5862\n",
      "Left-Right correspondences: 4168\n",
      "Triangulating...\n",
      " Reconstructed 16141 points and saved to reconstruction.ply\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Coordinate transformation from Blender to OpenCV\n",
    "def blender_to_opencv_transform():\n",
    "    return np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, -1, 0],\n",
    "        [0, 0, -1]\n",
    "    ])\n",
    "\n",
    "# Load grayscale images from three cameras\n",
    "def load_grayscale_image(path):\n",
    "    img = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    return img\n",
    "\n",
    "img_left = load_grayscale_image(\"5/Camera_Left.png\")\n",
    "img_front = load_grayscale_image(\"5/Camera_Front.png\")\n",
    "img_right = load_grayscale_image(\"5/Camera_Right.png\")\n",
    "\n",
    "# Extract SIFT features\n",
    "sift = cv.SIFT_create(nfeatures=20000, contrastThreshold=0.005, edgeThreshold=10, sigma=1.2)\n",
    "kpL, desL = sift.detectAndCompute(img_left, None)\n",
    "kpF, desF = sift.detectAndCompute(img_front, None)\n",
    "kpR, desR = sift.detectAndCompute(img_right, None)\n",
    "\n",
    "# Load camera parameters\n",
    "with open(\"5/camera_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "# Access camera parameter as dictionary\n",
    "def get_params(cam_name):\n",
    "    return params[cam_name]\n",
    "\n",
    "# Use extrinsic matrix directly\n",
    "def parse_blender_camera(cam):\n",
    "    extrinsic = np.array(cam[\"extrinsic\"])  # 4x4 Blender world-to-camera matrix\n",
    "    R_blender = extrinsic[:3, :3]\n",
    "    t_blender = extrinsic[:3, 3].reshape(3, 1)\n",
    "\n",
    "    R_transform = blender_to_opencv_transform()\n",
    "\n",
    "    R_world_to_cam = R_blender\n",
    "    R = R_transform @ R_world_to_cam\n",
    "\n",
    "    C_world = -R_world_to_cam.T @ t_blender\n",
    "    C = R_transform @ C_world\n",
    "\n",
    "    return R, C\n",
    "\n",
    "# Parse camera extrinsics\n",
    "camL = get_params(\"Camera_Left\")\n",
    "camF = get_params(\"Camera_Front\")\n",
    "camR = get_params(\"Camera_Right\")\n",
    "\n",
    "R_world_to_L, C_L = parse_blender_camera(camL)\n",
    "R_world_to_F, C_F = parse_blender_camera(camF)\n",
    "R_world_to_R, C_R = parse_blender_camera(camR)\n",
    "\n",
    "# Compute relative pose\n",
    "def compute_relative_pose(R1, C1, R2, C2):\n",
    "    R_rel = R2 @ R1.T\n",
    "    t_rel = R2 @ (C1 - C2)\n",
    "    return R_rel, t_rel\n",
    "\n",
    "R_F_to_L, t_F_to_L = compute_relative_pose(R_world_to_F, C_F, R_world_to_L, C_L)\n",
    "R_F_to_R, t_F_to_R = compute_relative_pose(R_world_to_F, C_F, R_world_to_R, C_R)\n",
    "\n",
    "# Intrinsics\n",
    "K = np.array([\n",
    "    [2666.6667, 0, 960.0],\n",
    "    [0, 2250.0, 540.0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# Projection matrix builder\n",
    "def build_projection(R_rel, t_rel):\n",
    "    return K @ np.hstack([R_rel, t_rel])\n",
    "\n",
    "# Projection matrices\n",
    "P_F = K @ np.hstack([np.eye(3), np.zeros((3, 1))])\n",
    "P_L = build_projection(R_F_to_L, t_F_to_L)\n",
    "P_R = build_projection(R_F_to_R, t_F_to_R)\n",
    "\n",
    "# Feature matcher and RANSAC filter\n",
    "def get_inliers(des1, des2, kp1, kp2):\n",
    "    bf = cv.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    good = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "    if not good:\n",
    "        return np.array([]), np.array([])\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in good])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in good])\n",
    "    F, mask = cv.findFundamentalMat(pts1, pts2, cv.FM_RANSAC, 1.0, 0.99)\n",
    "    if F is None or mask.sum() < 4:\n",
    "        return np.array([]), np.array([])\n",
    "    return pts1[mask.ravel() == 1], pts2[mask.ravel() == 1]\n",
    "\n",
    "# Match and triangulate\n",
    "print(\"Matching pairs...\")\n",
    "ptsF_L, ptsL_F = get_inliers(desF, desL, kpF, kpL)\n",
    "print(f\"Left-Front correspondences: {len(ptsL_F)}\")\n",
    "ptsF_R, ptsR_F = get_inliers(desF, desR, kpF, kpR)\n",
    "print(f\"Front-Right correspondences: {len(ptsF_R)}\")\n",
    "ptsL_R, ptsR_L = get_inliers(desL, desR, kpL, kpR)\n",
    "print(f\"Left-Right correspondences: {len(ptsL_R)}\")\n",
    "\n",
    "# Triangulate points\n",
    "def triangulate_points(P1, P2, pts1, pts2):\n",
    "    if len(pts1) < 4 or len(pts2) < 4:\n",
    "        return np.array([])\n",
    "    pts4d = cv.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    pts3d = (pts4d[:3] / pts4d[3]).T\n",
    "    valid = (pts3d[:, 2] > 0.1) & (pts3d[:, 2] < 10.0)\n",
    "    return pts3d[valid]\n",
    "\n",
    "print(\"Triangulating...\")\n",
    "points_FL = triangulate_points(P_F, P_L, ptsF_L, ptsL_F)\n",
    "points_FR = triangulate_points(P_F, P_R, ptsF_R, ptsR_F)\n",
    "points_LR = triangulate_points(P_L, P_R, ptsL_R, ptsR_L)\n",
    "\n",
    "# Combine points\n",
    "all_points = np.vstack([points_FL, points_FR, points_LR])\n",
    "\n",
    "# Transform to Blender coordinates\n",
    "transform = blender_to_opencv_transform().T\n",
    "points_blender = all_points @ transform\n",
    "\n",
    "# Create and save colored point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_blender)\n",
    "red_color = np.tile([1.0, 0.0, 0.0], (len(points_blender), 1))\n",
    "pcd.colors = o3d.utility.Vector3dVector(red_color)\n",
    "\n",
    "o3d.io.write_point_cloud(\"reconstruction.ply\", pcd)\n",
    "print(f\" Reconstructed {len(points_blender)} points and saved to reconstruction.ply\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264185aa",
   "metadata": {},
   "source": [
    "### Adding Texiture from Front image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8b13ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] Write OBJ can not include triangle normals.\n",
      "Saved textured mesh with 16141 points\n"
     ]
    }
   ],
   "source": [
    "def create_textured_mesh(points, camera_params, texture_img_path):\n",
    "    # Convert 3D points into a point cloud and estimate normals\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.estimate_normals()\n",
    "    pcd.orient_normals_consistent_tangent_plane(k=15)\n",
    "\n",
    "    # Generate a mesh using Poisson surface reconstruction\n",
    "    mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)\n",
    "    mesh.remove_duplicated_vertices()\n",
    "    mesh.remove_degenerate_triangles()\n",
    "    mesh.remove_unreferenced_vertices()\n",
    "    mesh = mesh.filter_smooth_simple(number_of_iterations=1)\n",
    "\n",
    "    # Retrieve camera intrinsic and extrinsic parameters\n",
    "    K = camera_params['K']\n",
    "    R = camera_params['R']\n",
    "    T = camera_params['T'].reshape(3, 1)\n",
    "    width = camera_params['width']\n",
    "    height = camera_params['height']\n",
    "\n",
    "    # Project 3D mesh vertices into 2D image space\n",
    "    vertices = np.asarray(mesh.vertices)\n",
    "    cam_coords = (R @ vertices.T + T).T\n",
    "    fx, fy = K[0, 0], K[1, 1]\n",
    "    cx, cy = K[0, 2], K[1, 2]\n",
    "    x_proj = (fx * cam_coords[:, 0] / cam_coords[:, 2]) + cx\n",
    "    y_proj = (fy * cam_coords[:, 1] / cam_coords[:, 2]) + cy\n",
    "\n",
    "    # Compute UV texture coordinates (with U flipped for image alignment)\n",
    "    u = np.clip(1 - (x_proj / width), 0, 1)\n",
    "    v = np.clip(y_proj / height, 0, 1)\n",
    "\n",
    "    # Assign UV coordinates per triangle vertex\n",
    "    triangle_uvs = []\n",
    "    for tri in np.asarray(mesh.triangles):\n",
    "        triangle_uvs.extend([\n",
    "            [u[tri[0]], v[tri[0]]],\n",
    "            [u[tri[1]], v[tri[1]]],\n",
    "            [u[tri[2]], v[tri[2]]]\n",
    "        ])\n",
    "    mesh.triangle_uvs = o3d.utility.Vector2dVector(np.array(triangle_uvs))\n",
    "    mesh.triangle_material_ids = o3d.utility.IntVector([0] * len(mesh.triangles))\n",
    "\n",
    "    # Load and process the texture image\n",
    "    texture_img = o3d.io.read_image(texture_img_path)\n",
    "    texture_np = np.asarray(texture_img)\n",
    "    if texture_np.ndim == 3 and texture_np.shape[2] == 4:\n",
    "        texture_np = texture_np[:, :, :3]  # Remove alpha channel\n",
    "    elif texture_np.ndim == 2:\n",
    "        texture_np = np.stack([texture_np] * 3, axis=-1)  # Grayscale to RGB\n",
    "    texture_np = np.ascontiguousarray(texture_np)  # Ensure memory layout\n",
    "    mesh.textures = [o3d.geometry.Image(texture_np)]\n",
    "\n",
    "    return mesh\n",
    "\n",
    "# Define intrinsic and extrinsic parameters for the front camera\n",
    "front_cam_params = {\n",
    "    'K': K,\n",
    "    'R': np.eye(3),  # No rotation (identity)\n",
    "    'T': np.zeros((3, 1)),  # No translation\n",
    "    'width': 1920,\n",
    "    'height': 1080\n",
    "}\n",
    "\n",
    "# Generate the textured mesh using front camera projection and texture\n",
    "textured_mesh = create_textured_mesh(\n",
    "    points_blender,\n",
    "    front_cam_params,\n",
    "    \"5/Camera_Front.png\"\n",
    ")\n",
    "\n",
    "# Post-process mesh normals for better visualization and export\n",
    "textured_mesh.compute_vertex_normals()\n",
    "textured_mesh.compute_triangle_normals()\n",
    "\n",
    "# Save the textured mesh  OBJ formats with UVs\n",
    "o3d.io.write_triangle_mesh(\"reconstruction_textured.obj\", textured_mesh, write_triangle_uvs=True)\n",
    "\n",
    "print(f\"Saved textured mesh with {len(all_points)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2841ac",
   "metadata": {},
   "source": [
    "### Reprojection Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4ae61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching pairs...\n",
      "Left-Front matches: 6111\n",
      "Front-Right matches: 5862\n",
      "Left-Right matches: 4168\n",
      "Triangulating...\n",
      "Reconstructed 16141 points and saved to reconstruction.ply\n",
      "\n",
      " Evaluating Reprojection Errors:\n",
      "Front-Left reprojection error: 0.06 pixels\n",
      "Front-Right reprojection error: 0.06 pixels\n",
      "Left-Right reprojection error: 0.06 pixels\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import json\n",
    "\n",
    "# Coordinate transformation from Blender to OpenCV\n",
    "def blender_to_opencv_transform():\n",
    "    return np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, -1, 0],\n",
    "        [0, 0, -1]\n",
    "    ])\n",
    "\n",
    "# Load grayscale image\n",
    "def load_grayscale_image(path):\n",
    "    img = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\" Image not found: {path}\")\n",
    "    return img\n",
    "\n",
    "# Load images\n",
    "img_left = load_grayscale_image(\"5/Camera_Left.png\")\n",
    "img_front = load_grayscale_image(\"5/Camera_Front.png\")\n",
    "img_right = load_grayscale_image(\"5/Camera_Right.png\")\n",
    "\n",
    "# SIFT features\n",
    "sift = cv.SIFT_create(nfeatures=20000, contrastThreshold=0.005, edgeThreshold=10, sigma=1.2)\n",
    "kpL, desL = sift.detectAndCompute(img_left, None)\n",
    "kpF, desF = sift.detectAndCompute(img_front, None)\n",
    "kpR, desR = sift.detectAndCompute(img_right, None)\n",
    "\n",
    "# Load camera parameters\n",
    "with open(\"5/camera_params.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "def get_params(cam_name):\n",
    "    return params[cam_name]\n",
    "\n",
    "# Convert Blender camera to OpenCV format\n",
    "def parse_blender_camera(cam):\n",
    "    extrinsic = np.array(cam[\"extrinsic\"])\n",
    "    R_blender = extrinsic[:3, :3]\n",
    "    t_blender = extrinsic[:3, 3].reshape(3, 1)\n",
    "    R_transform = blender_to_opencv_transform()\n",
    "    R = R_transform @ R_blender\n",
    "    C_world = -R_blender.T @ t_blender\n",
    "    C = R_transform @ C_world\n",
    "    return R, C\n",
    "\n",
    "# Camera poses\n",
    "camL = get_params(\"Camera_Left\")\n",
    "camF = get_params(\"Camera_Front\")\n",
    "camR = get_params(\"Camera_Right\")\n",
    "R_world_to_L, C_L = parse_blender_camera(camL)\n",
    "R_world_to_F, C_F = parse_blender_camera(camF)\n",
    "R_world_to_R, C_R = parse_blender_camera(camR)\n",
    "\n",
    "# Relative poses\n",
    "def compute_relative_pose(R1, C1, R2, C2):\n",
    "    R_rel = R2 @ R1.T\n",
    "    t_rel = R2 @ (C1 - C2)\n",
    "    return R_rel, t_rel\n",
    "\n",
    "R_F_to_L, t_F_to_L = compute_relative_pose(R_world_to_F, C_F, R_world_to_L, C_L)\n",
    "R_F_to_R, t_F_to_R = compute_relative_pose(R_world_to_F, C_F, R_world_to_R, C_R)\n",
    "R_L_to_R, t_L_to_R = compute_relative_pose(R_world_to_L, C_L, R_world_to_R, C_R)\n",
    "\n",
    "# Intrinsics\n",
    "K = np.array([\n",
    "    [2666.6667, 0, 960.0],\n",
    "    [0, 2250.0, 540.0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# Build projection matrix\n",
    "def build_projection(R_rel, t_rel):\n",
    "    return K @ np.hstack([R_rel, t_rel])\n",
    "\n",
    "# Projections\n",
    "P_F = K @ np.hstack([np.eye(3), np.zeros((3, 1))])\n",
    "P_L = build_projection(R_F_to_L, t_F_to_L)\n",
    "P_R = build_projection(R_F_to_R, t_F_to_R)\n",
    "P_L_ref = K @ np.hstack([np.eye(3), np.zeros((3, 1))])  # For Left-Right pair\n",
    "P_R_from_L = build_projection(R_L_to_R, t_L_to_R)\n",
    "\n",
    "# Feature matching and RANSAC\n",
    "def get_inliers(des1, des2, kp1, kp2):\n",
    "    bf = cv.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    good = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "    if not good:\n",
    "        return np.array([]), np.array([])\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in good])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in good])\n",
    "    F, mask = cv.findFundamentalMat(pts1, pts2, cv.FM_RANSAC, 1.0, 0.99)\n",
    "    if F is None or mask.sum() < 4:\n",
    "        return np.array([]), np.array([])\n",
    "    return pts1[mask.ravel() == 1], pts2[mask.ravel() == 1]\n",
    "\n",
    "# Triangulation with filtering\n",
    "def triangulate_points(P1, P2, pts1, pts2):\n",
    "    if len(pts1) < 4 or len(pts2) < 4:\n",
    "        return np.array([]), np.array([])\n",
    "    pts4d = cv.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    pts3d = (pts4d[:3] / pts4d[3]).T\n",
    "    valid = (pts3d[:, 2] > 0.1) & (pts3d[:, 2] < 10.0)\n",
    "    return pts3d[valid], pts1[valid]\n",
    "\n",
    "# Match and triangulate\n",
    "print(\"Matching pairs...\")\n",
    "ptsF_L, ptsL_F = get_inliers(desF, desL, kpF, kpL)\n",
    "print(f\"Left-Front matches: {len(ptsF_L)}\")\n",
    "ptsF_R, ptsR_F = get_inliers(desF, desR, kpF, kpR)\n",
    "print(f\"Front-Right matches: {len(ptsF_R)}\")\n",
    "ptsL_R, ptsR_L = get_inliers(desL, desR, kpL, kpR)\n",
    "print(f\"Left-Right matches: {len(ptsL_R)}\")\n",
    "\n",
    "print(\"Triangulating...\")\n",
    "points_FL, ptsF_L_valid = triangulate_points(P_F, P_L, ptsF_L, ptsL_F)\n",
    "points_FR, ptsF_R_valid = triangulate_points(P_F, P_R, ptsF_R, ptsR_F)\n",
    "points_LR, ptsL_R_valid = triangulate_points(P_L_ref, P_R_from_L, ptsL_R, ptsR_L)\n",
    "\n",
    "# Combine and transform points to Blender space\n",
    "all_points = np.vstack([points_FL, points_FR, points_LR])\n",
    "transform = blender_to_opencv_transform().T\n",
    "points_blender = all_points @ transform\n",
    "\n",
    "# Save point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_blender)\n",
    "red_color = np.tile([1.0, 0.0, 0.0], (len(points_blender), 1))\n",
    "pcd.colors = o3d.utility.Vector3dVector(red_color)\n",
    "o3d.io.write_point_cloud(\"reconstruction.ply\", pcd)\n",
    "print(f\"Reconstructed {len(points_blender)} points and saved to reconstruction.ply\")\n",
    "\n",
    "# Reprojection error evaluation\n",
    "def compute_reprojection_error(pts_3d, pts_2d, R, C, K):\n",
    "    if pts_3d.shape[0] == 0 or pts_2d.shape[0] == 0:\n",
    "        return None, None\n",
    "    t = -R @ C\n",
    "    rvec, _ = cv.Rodrigues(R)\n",
    "    projected, _ = cv.projectPoints(pts_3d, rvec, t, K, None)\n",
    "    projected = projected.squeeze()\n",
    "    error = np.linalg.norm(projected - pts_2d, axis=1)\n",
    "    return error.mean(), error\n",
    "\n",
    "def evaluate_reconstruction():\n",
    "    print(\"\\n Evaluating Reprojection Errors:\")\n",
    "\n",
    "    err_FL, _ = compute_reprojection_error(points_FL, ptsF_L_valid, np.eye(3), np.zeros((3, 1)), K)\n",
    "    if err_FL is not None:\n",
    "        print(f\"Front-Left reprojection error: {err_FL:.2f} pixels\")\n",
    "\n",
    "    err_FR, _ = compute_reprojection_error(points_FR, ptsF_R_valid, np.eye(3), np.zeros((3, 1)), K)\n",
    "    if err_FR is not None:\n",
    "        print(f\"Front-Right reprojection error: {err_FR:.2f} pixels\")\n",
    "\n",
    "    err_LR, _ = compute_reprojection_error(points_LR, ptsL_R_valid, np.eye(3), np.zeros((3, 1)), K)\n",
    "    if err_LR is not None:\n",
    "        print(f\"Left-Right reprojection error: {err_LR:.2f} pixels\")\n",
    "\n",
    "evaluate_reconstruction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d753fd",
   "metadata": {},
   "source": [
    "### Calibrated 3D Face Reconstruction and Evaluation from Stereo Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73b2514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Matching pairs...\n",
      "Left-Front correspondences: 6111\n",
      "Front-Right correspondences: 5862\n",
      "Left-Right correspondences: 4168\n",
      "Triangulating...\n",
      "Reconstructed 16141 points and saved to reconstruction.ply\n",
      "Saved camera positions to camera_positions.json\n",
      "\n",
      "Evaluating reconstruction quality...\n",
      "Camera_Left: Generated 848931 ground truth points\n",
      "Camera_Right: Generated 720304 ground truth points\n",
      "Camera_Front: Generated 773210 ground truth points\n",
      "Total ground truth points: 2342445\n",
      "Downsampling ground truth points for efficiency\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluation Metrics (units: centimeters)\n",
      "------------------------------------------------------------\n",
      "Matched Points      : 16141/16141\n",
      "Mean Error          : 0.042 cm\n",
      "Median Error        : 0.036 cm\n",
      "RMSE                : 0.061 cm\n",
      "Std Dev             : 0.044 cm\n",
      "Inlier Ratio (<1mm) : 96.52%\n",
      "Inlier Ratio (<1cm) : 99.96%\n",
      "Inlier Ratio (<5cm) : 100.00%\n",
      "Coverage            : 100.00%\n",
      "------------------------------------------------------------\n",
      "Saved evaluation point cloud to evaluation.ply\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import json\n",
    "import os\n",
    "import pyexr\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def blender_to_opencv_transform():\n",
    "    return np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, -1, 0],\n",
    "        [0, 0, -1]\n",
    "    ])\n",
    "\n",
    "def load_grayscale_image(path):\n",
    "    img = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    return img\n",
    "\n",
    "def load_exr_depth(path):\n",
    "    try:\n",
    "        exr = pyexr.read(path)\n",
    "        if exr.ndim == 3:\n",
    "            return exr[:, :, 0]\n",
    "        return exr\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def backproject_depth_map(depth_map, K, cam_to_world):\n",
    "    h, w = depth_map.shape\n",
    "    u, v = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    valid_depth = (depth_map > 0) & (depth_map < 100) & np.isfinite(depth_map)\n",
    "\n",
    "    if not np.any(valid_depth):\n",
    "        return np.array([])\n",
    "\n",
    "    u_valid = u[valid_depth]\n",
    "    v_valid = v[valid_depth]\n",
    "    depths = depth_map[valid_depth]\n",
    "\n",
    "    x = (u_valid - K[0, 2]) * depths / K[0, 0]\n",
    "    y = (v_valid - K[1, 2]) * depths / K[1, 1]\n",
    "    z = depths\n",
    "\n",
    "    points_cam = np.vstack([x, y, z]).T\n",
    "    points_cam_hom = np.hstack([points_cam, np.ones((len(points_cam), 1))])\n",
    "    points_world = (cam_to_world @ points_cam_hom.T).T[:, :3]\n",
    "    return points_world\n",
    "\n",
    "def evaluate_reconstruction(recon_points, camera_params):\n",
    "    \"\"\"Evaluate reconstructed points against ground truth depth maps\"\"\"\n",
    "    all_gt_points = []\n",
    "    \n",
    "    for cam_name, params in camera_params.items():\n",
    "        try:\n",
    "            # Get camera parameters\n",
    "            intrinsic = np.array(params['intrinsic'])\n",
    "            extrinsic = np.array(params['extrinsic'])\n",
    "            depth_path = os.path.join(\"0\", params['depth_path'])\n",
    "            \n",
    "            # Load depth map\n",
    "            depth_map = load_exr_depth(depth_path)\n",
    "            if depth_map is None:\n",
    "                continue\n",
    "                \n",
    "            # Convert to single channel \n",
    "            if depth_map.ndim == 3:\n",
    "                depth_map = depth_map[:, :, 0]\n",
    "                \n",
    "            # Compute camera to world transform\n",
    "            cam_to_world = np.linalg.inv(extrinsic)\n",
    "            \n",
    "            # Backproject depth to 3D points\n",
    "            gt_points = backproject_depth_map(depth_map, intrinsic, cam_to_world)\n",
    "            if len(gt_points) > 0:\n",
    "                all_gt_points.append(gt_points)\n",
    "                print(f\"{cam_name}: Generated {len(gt_points)} ground truth points\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {cam_name}: {e}\")\n",
    "\n",
    "    if not all_gt_points:\n",
    "        print(\"No valid ground truth points\")\n",
    "        return None\n",
    "        \n",
    "    all_gt_points = np.vstack(all_gt_points)\n",
    "    print(f\"Total ground truth points: {len(all_gt_points)}\")\n",
    "\n",
    "    # Downsample if too many points\n",
    "    if len(all_gt_points) > 1000000:\n",
    "        print(\"Downsampling ground truth points for efficiency\")\n",
    "        indices = np.random.choice(len(all_gt_points), 1000000, replace=False)\n",
    "        all_gt_points = all_gt_points[indices]\n",
    "\n",
    "    # Build KDTree for evaluation\n",
    "    gt_tree = KDTree(all_gt_points)\n",
    "    distances, _ = gt_tree.query(recon_points, k=1, workers=-1)\n",
    "    \n",
    "    # Filter valid matches\n",
    "    valid_mask = distances < 0.1  # 1 cm threshold\n",
    "    valid_distances = distances[valid_mask]\n",
    "    \n",
    "    if len(valid_distances) == 0:\n",
    "        print(\"No valid matches found\")\n",
    "        return None\n",
    "        \n",
    " # Compute metrics with finer inlier thresholds\n",
    "    metrics = {\n",
    "        'mean_error': np.mean(valid_distances),\n",
    "        'median_error': np.median(valid_distances),\n",
    "        'std_error': np.std(valid_distances),\n",
    "        'rmse': np.sqrt(np.mean(np.square(valid_distances))),\n",
    "        'inlier_ratio_1mm': np.mean(valid_distances < 0.001),\n",
    "        'inlier_ratio_1cm': np.mean(valid_distances < 0.01),\n",
    "        'inlier_ratio_5cm': np.mean(valid_distances < 0.05),\n",
    "        'coverage': len(valid_distances) / len(recon_points),\n",
    "        'points_matched': len(valid_distances),\n",
    "        'total_points': len(recon_points)\n",
    "    }\n",
    "\n",
    "    # Print results in centimeters \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Evaluation Metrics (units: centimeters)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Matched Points':20s}: {metrics['points_matched']}/{metrics['total_points']}\")\n",
    "    print(f\"{'Mean Error':20s}: {metrics['mean_error'] * 100:.3f} cm\")\n",
    "    print(f\"{'Median Error':20s}: {metrics['median_error'] * 100:.3f} cm\")\n",
    "    print(f\"{'RMSE':20s}: {metrics['rmse'] * 100:.3f} cm\")\n",
    "    print(f\"{'Std Dev':20s}: {metrics['std_error'] * 100:.3f} cm\")\n",
    "    print(f\"{'Inlier Ratio (<1mm)':20s}: {metrics['inlier_ratio_1mm'] * 100:.2f}%\")\n",
    "    print(f\"{'Inlier Ratio (<1cm)':20s}: {metrics['inlier_ratio_1cm'] * 100:.2f}%\")\n",
    "    print(f\"{'Inlier Ratio (<5cm)':20s}: {metrics['inlier_ratio_5cm'] * 100:.2f}%\")\n",
    "    print(f\"{'Coverage':20s}: {metrics['coverage'] * 100:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Main reconstruction pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Load images\n",
    "    img_left = load_grayscale_image(\"5/Camera_Left.png\")\n",
    "    img_front = load_grayscale_image(\"5/Camera_Front.png\")\n",
    "    img_right = load_grayscale_image(\"5/Camera_Right.png\")\n",
    "\n",
    "    # Detect features\n",
    "    sift = cv.SIFT_create(nfeatures=20000, contrastThreshold=0.005, edgeThreshold=10, sigma=1.2)\n",
    "    kpL, desL = sift.detectAndCompute(img_left, None)\n",
    "    kpF, desF = sift.detectAndCompute(img_front, None)\n",
    "    kpR, desR = sift.detectAndCompute(img_right, None)\n",
    "\n",
    "    # Load camera parameters\n",
    "    with open(\"5/camera_params.json\", \"r\") as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    def get_params(cam_name):\n",
    "        return params[cam_name]\n",
    "\n",
    "    def parse_blender_camera(cam):\n",
    "        extrinsic = np.array(cam[\"extrinsic\"])\n",
    "        R_blender = extrinsic[:3, :3]\n",
    "        t_blender = extrinsic[:3, 3].reshape(3, 1)\n",
    "\n",
    "        R_transform = blender_to_opencv_transform()\n",
    "        R = R_transform @ R_blender\n",
    "        C_world = -R_blender.T @ t_blender\n",
    "        C = R_transform @ C_world\n",
    "\n",
    "        return R, C, extrinsic  # extrinsic is Blender world-to-camera\n",
    "\n",
    "    camL = get_params(\"Camera_Left\")\n",
    "    camF = get_params(\"Camera_Front\")\n",
    "    camR = get_params(\"Camera_Right\")\n",
    "\n",
    "    R_world_to_L, C_L, extrinsic_L = parse_blender_camera(camL)\n",
    "    R_world_to_F, C_F, extrinsic_F = parse_blender_camera(camF)\n",
    "    R_world_to_R, C_R, extrinsic_R = parse_blender_camera(camR)\n",
    "\n",
    "    def compute_relative_pose(R1, C1, R2, C2):\n",
    "        R_rel = R2 @ R1.T\n",
    "        t_rel = R2 @ (C1 - C2)\n",
    "        return R_rel, t_rel\n",
    "\n",
    "    R_F_to_L, t_F_to_L = compute_relative_pose(R_world_to_F, C_F, R_world_to_L, C_L)\n",
    "    R_F_to_R, t_F_to_R = compute_relative_pose(R_world_to_F, C_F, R_world_to_R, C_R)\n",
    "\n",
    "    # Use ACTUAL intrinsics from camera parameters\n",
    "    K_front = np.array(camF[\"intrinsic\"])\n",
    "    K_left = np.array(camL[\"intrinsic\"])\n",
    "    K_right = np.array(camR[\"intrinsic\"])\n",
    "\n",
    "    def build_projection(R_rel, t_rel, K):\n",
    "        return K @ np.hstack([R_rel, t_rel])\n",
    "\n",
    "    P_F = K_front @ np.hstack([np.eye(3), np.zeros((3, 1))])\n",
    "    P_L = build_projection(R_F_to_L, t_F_to_L, K_left)\n",
    "    P_R = build_projection(R_F_to_R, t_F_to_R, K_right)\n",
    "\n",
    "    def get_inliers(des1, des2, kp1, kp2):\n",
    "        bf = cv.BFMatcher()\n",
    "        matches = bf.knnMatch(des1, des2, k=2)\n",
    "        good = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "        if not good:\n",
    "            return np.array([]), np.array([])\n",
    "        pts1 = np.float32([kp1[m.queryIdx].pt for m in good])\n",
    "        pts2 = np.float32([kp2[m.trainIdx].pt for m in good])\n",
    "        F, mask = cv.findFundamentalMat(pts1, pts2, cv.FM_RANSAC, 1.0, 0.99)\n",
    "        if F is None or mask.sum() < 4:\n",
    "            return np.array([]), np.array([])\n",
    "        return pts1[mask.ravel() == 1], pts2[mask.ravel() == 1]\n",
    "\n",
    "    print(\"Matching pairs...\")\n",
    "    ptsF_L, ptsL_F = get_inliers(desF, desL, kpF, kpL)\n",
    "    print(f\"Left-Front correspondences: {len(ptsL_F)}\")\n",
    "    ptsF_R, ptsR_F = get_inliers(desF, desR, kpF, kpR)\n",
    "    print(f\"Front-Right correspondences: {len(ptsF_R)}\")\n",
    "    ptsL_R, ptsR_L = get_inliers(desL, desR, kpL, kpR)\n",
    "    print(f\"Left-Right correspondences: {len(ptsL_R)}\")\n",
    "\n",
    "    def triangulate_points(P1, P2, pts1, pts2):\n",
    "        if len(pts1) < 4 or len(pts2) < 4:\n",
    "            return np.array([])\n",
    "        pts4d = cv.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "        pts3d = (pts4d[:3] / pts4d[3]).T\n",
    "        valid = (pts3d[:, 2] > 0.1) & (pts3d[:, 2] < 10.0)\n",
    "        return pts3d[valid]\n",
    "\n",
    "    print(\"Triangulating...\")\n",
    "    points_FL = triangulate_points(P_F, P_L, ptsF_L, ptsL_F)\n",
    "    points_FR = triangulate_points(P_F, P_R, ptsF_R, ptsR_F)\n",
    "    points_LR = triangulate_points(P_L, P_R, ptsL_R, ptsR_L)\n",
    "\n",
    "    all_points = np.vstack([points_FL, points_FR, points_LR])\n",
    "\n",
    "    # Transform from OpenCV front camera frame to Blender world\n",
    "    n = all_points.shape[0]\n",
    "    points_hom = np.hstack([all_points, np.ones((n, 1))])\n",
    "    M_front_inv = np.linalg.inv(extrinsic_F)\n",
    "    points_world = (M_front_inv @ points_hom.T).T[:, :3]\n",
    "\n",
    "    # Create and save point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points_world)\n",
    "    red_color = np.tile([1.0, 0.0, 0.0], (n, 1))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(red_color)\n",
    "\n",
    "    o3d.io.write_point_cloud(\"reconstruction.ply\", pcd)\n",
    "    print(f\"Reconstructed {n} points and saved to reconstruction.ply\")\n",
    "\n",
    "    # Save Blender-space camera centers\n",
    "    def camera_center_from_extrinsic(E):\n",
    "        R = E[:3, :3]\n",
    "        t = E[:3, 3]\n",
    "        return (-R.T @ t).tolist()\n",
    "\n",
    "    camera_positions = {\n",
    "        \"Camera_Front\": camera_center_from_extrinsic(extrinsic_F),\n",
    "        \"Camera_Left\": camera_center_from_extrinsic(extrinsic_L),\n",
    "        \"Camera_Right\": camera_center_from_extrinsic(extrinsic_R)\n",
    "    }\n",
    "\n",
    "    with open(\"camera_positions.json\", \"w\") as f:\n",
    "        json.dump(camera_positions, f)\n",
    "    print(\"Saved camera positions to camera_positions.json\")\n",
    "    \n",
    "    # Evaluate reconstruction\n",
    "    print(\"\\nEvaluating reconstruction quality...\")\n",
    "    metrics = evaluate_reconstruction(points_world, params)\n",
    "        # Create visualization point cloud\n",
    "    if metrics:\n",
    "        # Create reconstruction point cloud (red)\n",
    "        recon_pcd = o3d.geometry.PointCloud()\n",
    "        recon_pcd.points = o3d.utility.Vector3dVector(points_world)\n",
    "        recon_colors = np.tile([1.0, 0.0, 0.0], (len(points_world), 1))  # Red\n",
    "        recon_pcd.colors = o3d.utility.Vector3dVector(recon_colors)\n",
    "\n",
    "        # Camera positions (blue)\n",
    "        cam_points = []\n",
    "        for cam_name in [\"Camera_Front\", \"Camera_Left\", \"Camera_Right\"]:\n",
    "            cam_points.append(camera_center_from_extrinsic(np.array(params[cam_name][\"extrinsic\"])))\n",
    "        cam_points = np.array(cam_points)\n",
    "\n",
    "        cam_pcd = o3d.geometry.PointCloud()\n",
    "        cam_pcd.points = o3d.utility.Vector3dVector(cam_points)\n",
    "        cam_colors = np.tile([0.0, 0.0, 1.0], (len(cam_points), 1))  # Blue\n",
    "        cam_pcd.colors = o3d.utility.Vector3dVector(cam_colors)\n",
    "\n",
    "        # Save evaluation (reconstruction + cameras)\n",
    "        combined_pcd = recon_pcd + cam_pcd\n",
    "        o3d.io.write_point_cloud(\"evaluation.ply\", combined_pcd)\n",
    "        print(\"Saved evaluation point cloud to evaluation.ply\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
